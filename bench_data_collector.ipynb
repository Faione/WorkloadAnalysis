{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "be893bbc-729e-437b-9213-a769d8107813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 配置\n",
    "TEST_COUNT=\"03\"\n",
    "\n",
    "META_DATA_FILE = 'data/bench_test{}/stress.log'.format(TEST_COUNT)\n",
    "META_DATA_OUT_FILE = 'data/bench_data{}/meta.csv'.format(TEST_COUNT)\n",
    "PROMETHEUS_URL = 'http://10.208.130.243:9090'\n",
    "DATA_STEP = 2\n",
    "QUERYS = {\n",
    "    \"host_cpu_usage_total\": ('range_series','clamp_min(1 - avg(rate(node_cpu_seconds_total{cpu=\"131\",mode=\"idle\"}[1m])),0)'),\n",
    "    \"host_cpu_usage_user\": ('range_series','avg by(mode)(rate(node_cpu_seconds_total{cpu=\"131\",mode=\"user\"}[1m]))'),\n",
    "    \"host_cpu_usage_system\": ('range_series','avg by(mode)(rate(node_cpu_seconds_total{cpu=\"131\",mode=\"system\"}[1m]))'),\n",
    "    \"host_cpu_usage_iowait\": ('range_series','avg by(mode)(rate(node_cpu_seconds_total{cpu=\"131\",mode=\"iowait\"}[1m]))'),\n",
    "    \"host_mem_usage_total\": ('range_series','node_memory_MemTotal_bytes - node_memory_MemFree_bytes'),\n",
    "    \"host_mem_bandwith_occupied\": ('range_series','rate(resctrl_mem_bandwidth_local_bytes{group=\"global\",numa=\"2\"}[1m])'),\n",
    "    \"host_context_switche\": ('range_series','rate(node_context_switches_total{instance=\"worknode:9100\", job=\"node_exporter\"}[1m])'),\n",
    "    \"host_intr\": ('range_series','rate(node_intr_total{}[1m])'),\n",
    "    \"host_load1\": ('range_series','node_load1 / on (instance) (count by (instance) (node_cpu_seconds_total{mode=\"idle\"}))'),\n",
    "\n",
    "    \"vm_cpu_usage_system\": ('range_series','clamp_min(rate(libvirt_domain_cpu_time_sys{domain=\"podman-alpine317_default\", instance=\"worknode:9177\", job=\"libvirt_exporter\"}[1m]), 0) / 1000'),\n",
    "    \"vm_cpu_usage_user\": ('range_series','clamp_min(rate(libvirt_domain_cpu_time_user{domain=\"podman-alpine317_default\", instance=\"worknode:9177\", job=\"libvirt_exporter\"}[1m]), 0) / 1000'),\n",
    "    \"vm_cpu_usage_total\": ('range_series','sum(rate(libvirt_domain_vcpu_time_total{domain=\"podman-alpine317_default\", instance=\"worknode:9177\", job=\"libvirt_exporter\"}[1m])) by(domain) / 1000'),\n",
    "    \"vm_mem_usage\": ('range_series','libvirt_domain_memory_stats_used_percent'),\n",
    "    \"vm_mem_bandwith_occupied\": ('range_series','rate(resctrl_mem_bandwidth_local_bytes{group!=\"global\",numa=\"2\"}[1m])'),\n",
    "    \"vm_cpu_llc_occupied\": ('range_series', 'sum( resctrl_llc_occupancy_bytes{group!=\"global\"} / on(numa) group_left resctrl_llc_occupancy_bytes{group=\"global\"})'),\n",
    "    \"vm_mkpi\": ('range_series','1000 * clamp_min(idelta(libvirt_domain_perf_count{domain=\"podman-alpine317_default\",instance=\"worknode:9177\", job=\"libvirt_exporter\",event=\"cache_misses\"}[1m]), 0)/on(domain) clamp_min(idelta(libvirt_domain_perf_count{domain=\"podman-alpine317_default\", instance=\"worknode:9177\", job=\"libvirt_exporter\", event=\"instructions\"}[1m]), 0)'),\n",
    "    \"vm_network_receive\": ('range_series','sum(rate(libvirt_domain_interface_stats_receive_bytes_total{domain=\"podman-alpine317_default\", instance=\"worknode:9177\", job=\"libvirt_exporter\"}[1m])) by(domain, instance) / 1024'),\n",
    "    \"vm_network_transmit\": ('range_series','sum(rate(libvirt_domain_interface_stats_transmit_bytes_total{domain=\"podman-alpine317_default\", instance=\"worknode:9177\", job=\"libvirt_exporter\"}[1m])) by(domain, instance) / 1024'),\n",
    "\n",
    "    \"application_p99\": ('point_bucket','envoy_redis_command_set_latency_bucket + envoy_redis_command_get_latency_bucket - (envoy_redis_command_set_latency_bucket offset {}s + envoy_redis_command_get_latency_bucket offset {}s)'),\n",
    "    \"application_qps\": ('range_series','rate(envoy_redis_command_get_total[1m]) + rate(envoy_redis_command_set_total[1m])'),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_data_save_to_csv():\n",
    "    with open(META_DATA_OUT_FILE, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            workload_start = row['workload_start']\n",
    "            workload_end = row['workload_end']\n",
    "            bench_params = row['params']\n",
    "            exec = row['exec']\n",
    "\n",
    "            \n",
    "            start =  float(workload_start)\n",
    "            end = float(workload_end)\n",
    "\n",
    "            series_cdf = []\n",
    "            bucket_cdf = []\n",
    "            for metric_name,(kind, query) in QUERYS.items():\n",
    "                # 查询 prometheus\n",
    "                if kind == \"range_series\":\n",
    "                    params = {\n",
    "                        'query': query,\n",
    "                        'start': start,\n",
    "                        'end': end,\n",
    "                        'step': DATA_STEP\n",
    "                    }\n",
    "                    response = requests.get(f'{PROMETHEUS_URL}/api/v1/query_range', params=params)\n",
    "                    response.raise_for_status()  # Raise exception if invalid response\n",
    "                    data = response.json()['data']['result']\n",
    "                    \n",
    "                    for metric_data in data:\n",
    "                        times, values = zip(*metric_data['values'])\n",
    "                        df = pd.DataFrame({\n",
    "                            'timestamp': times,\n",
    "                            metric_name: values\n",
    "                        })\n",
    "                        #df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "                        df['timestamp'] = df['timestamp'].astype(int)\n",
    "                        df.set_index('timestamp', inplace=True)\n",
    "                        series_cdf.append(df)\n",
    "                elif kind == \"point_bucket\":\n",
    "                    params = {\n",
    "                        'query': query.format(int(end-start),int(end-start)),\n",
    "                        'time': end,\n",
    "                    }\n",
    "                    response = requests.get(f'{PROMETHEUS_URL}/api/v1/query', params=params)\n",
    "                    response.raise_for_status()  # Raise exception if invalid response\n",
    "                    data = response.json()['data']['result']\n",
    "                    \n",
    "                    x_values = []\n",
    "                    y_values = []\n",
    "\n",
    "                    for result in data:\n",
    "                        if 'metric' in result and 'le' in result['metric'] and result['metric']['le'] != '+Inf':\n",
    "                            x_values.append(float(result['metric']['le']))\n",
    "                            y_values.append(float(result['value'][1]))\n",
    "                    df = pd.DataFrame({\n",
    "                        'timebucket': x_values,\n",
    "                        metric_name: y_values\n",
    "                    })\n",
    "                    df = df.sort_values(by='timebucket')\n",
    "                    df[metric_name] = df[metric_name] / df[metric_name].max()\n",
    "                    df.to_csv('data/bench_data{}/{}-bucket-{}-{}.csv'.format(TEST_COUNT,exec,metric_name,bench_params), index=False)\n",
    "\n",
    "            if len(series_cdf) > 0:\n",
    "                merged_range_df = pd.concat(series_cdf, axis=1)\n",
    "                merged_range_df.reset_index(inplace=True)\n",
    "                merged_range_df.to_csv('data/bench_data{}/{}-series-{}.csv'.format(TEST_COUNT,exec,bench_params), index=False)\n",
    "\n",
    "\n",
    "def stress_data_deal():\n",
    "    with open(META_DATA_FILE, 'r') as file:\n",
    "        content = file.read()\n",
    "        pattern = r\"(start|end) time (\\d+\\.\\d+) file: ./data/(\\w+)-(exec\\d+)-([\\w\\-]+)\"\n",
    "        data =  re.findall(pattern, content)\n",
    "    \n",
    "    grouped_data = {}\n",
    "\n",
    "    for entry in data:\n",
    "        action, time, task_type, exec_name, load = entry\n",
    "        if exec_name == \"exec70\":\n",
    "            break\n",
    "        if exec_name not in grouped_data:\n",
    "            grouped_data[exec_name] = {\"workload_start\": None, \"workload_end\": None, \n",
    "                                       \"stress_start\": None, \"stress_end\": None, \n",
    "                                       \"params\": load}\n",
    "        \n",
    "        if task_type == \"workload\" and action == \"start\":\n",
    "            grouped_data[exec_name][\"workload_start\"] = time\n",
    "        elif task_type == \"workload\" and action == \"end\":\n",
    "            grouped_data[exec_name][\"workload_end\"] = time\n",
    "        elif task_type == \"stress\" and action == \"start\":\n",
    "            grouped_data[exec_name][\"stress_start\"] = time\n",
    "        elif task_type == \"stress\" and action == \"end\":\n",
    "            grouped_data[exec_name][\"stress_end\"] = time\n",
    "    \n",
    "    result = []\n",
    "    for exec_name, timings in grouped_data.items():\n",
    "        exec_name = re.sub(r'(\\d+)', lambda x: f\"{int(x.group(1)):03}\", exec_name)\n",
    "        result.append((\n",
    "            timings[\"workload_start\"], timings[\"workload_end\"], \n",
    "            timings[\"stress_start\"], timings[\"stress_end\"], \n",
    "            timings[\"params\"], exec_name\n",
    "        ))\n",
    "    \n",
    "    with open(META_DATA_OUT_FILE, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"workload_start\",\"workload_end\", \"stress_start\",\"stress_end\",\"params\",\"exec\"])  # Write header\n",
    "        writer.writerows(result)\n",
    "\n",
    "def csvs_to_excel():\n",
    "    all_files = os.listdir('data/bench_data{}'.format(TEST_COUNT))\n",
    "    csv_files = [file for file in all_files if file.endswith('.csv')]\n",
    "\n",
    "    series = []\n",
    "    bucket = []\n",
    "    for file in csv_files:\n",
    "        if 'series' in file:\n",
    "            series.append(file)\n",
    "        elif 'bucket' in file:\n",
    "            bucket.append(file)\n",
    "            \n",
    "    with pd.ExcelWriter('data/bench_analysis{}/series.xlsx'.format(TEST_COUNT), engine='openpyxl') as writer:\n",
    "        for file in series:\n",
    "            df = pd.read_csv('data/bench_data{}/{}'.format(TEST_COUNT,file))\n",
    "            sheet_name = file.split('-')[0]\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    with pd.ExcelWriter('data/bench_analysis{}/bucket.xlsx'.format(TEST_COUNT), engine='openpyxl') as writer:\n",
    "        for file in bucket:\n",
    "            df = pd.read_csv('data/bench_data{}/{}'.format(TEST_COUNT,file))\n",
    "            sheet_name = file.split('-')[0]\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # # 根据日志提取数据\n",
    "    stress_data_deal()\n",
    "\n",
    "    # # 查询prometheus数据并保存到文件\n",
    "    fetch_data_save_to_csv()\n",
    "\n",
    "    csvs_to_excel()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bba1f6f-948d-4e1f-bdf7-9cfc8ac1994e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b2c4c1-c43a-4c96-99fa-c830f6a8fa6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
